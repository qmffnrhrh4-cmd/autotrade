"""
Self-Learning Reinforcement System
ìê¸° ê°•í™” í•™ìŠµ ì‹œìŠ¤í…œ

ëª¨ë“  ê±°ë˜ì—ì„œ í•™ìŠµí•˜ì—¬ ì „ëµì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ 
"""
import logging
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
import json
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class TradeExperience:
    """ê±°ë˜ ê²½í—˜ (ê°•í™”í•™ìŠµì˜ ê²½í—˜)"""
    trade_id: str
    timestamp: datetime
    stock_code: str
    stock_name: str

    # ìƒíƒœ (State)
    state: Dict[str, Any]  # ì§„ì… ì‹œ ì‹œì¥ ìƒíƒœ

    # í–‰ë™ (Action)
    action: Dict[str, Any]  # ì·¨í•œ í–‰ë™ (íŒŒë¼ë¯¸í„°)

    # ë³´ìƒ (Reward)
    reward: float  # ìˆ˜ìµë¥ 

    # ë‹¤ìŒ ìƒíƒœ (Next State)
    next_state: Optional[Dict[str, Any]] = None

    # ë©”íƒ€ë°ì´í„°
    duration_hours: float = 0.0
    max_drawdown: float = 0.0
    is_win: bool = False


@dataclass
class LearningStats:
    """í•™ìŠµ í†µê³„"""
    total_experiences: int = 0
    total_wins: int = 0
    total_losses: int = 0
    avg_reward: float = 0.0
    best_reward: float = -np.inf
    worst_reward: float = np.inf
    learning_episodes: int = 0
    last_updated: datetime = field(default_factory=datetime.now)


class SelfLearningSystem:
    """
    ìê¸° ê°•í™” í•™ìŠµ ì‹œìŠ¤í…œ

    ê¸°ëŠ¥:
    - Q-Learning ê¸°ë°˜ ì „ëµ í•™ìŠµ
    - ê²½í—˜ ë¦¬í”Œë ˆì´ (Experience Replay)
    - ìƒíƒœ-í–‰ë™ ê°€ì¹˜ í•™ìŠµ
    - íŒ¨í„´ ì¸ì‹ ë° ì˜ˆì¸¡
    - ì ì‘í˜• í•™ìŠµë¥ 
    """

    def __init__(
        self,
        db_path: str = "data/self_learning.json",
        memory_size: int = 10000,
        learning_rate: float = 0.1,
        discount_factor: float = 0.95
    ):
        """
        Args:
            db_path: í•™ìŠµ ë°ì´í„° ì €ì¥ ê²½ë¡œ
            memory_size: ê²½í—˜ ë©”ëª¨ë¦¬ í¬ê¸°
            learning_rate: í•™ìŠµë¥ 
            discount_factor: í• ì¸ ê³„ìˆ˜ (ë¯¸ë˜ ë³´ìƒ)
        """
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = 0.3  # íƒí—˜ ë¹„ìœ¨
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.05

        # ê²½í—˜ ë©”ëª¨ë¦¬ (Experience Replay)
        self.memory: deque = deque(maxlen=memory_size)

        # Q-í…Œì´ë¸” (ìƒíƒœ-í–‰ë™ ê°€ì¹˜)
        self.q_table: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))

        # ìƒíƒœ-í–‰ë™ ë°©ë¬¸ íšŸìˆ˜
        self.visit_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))

        # í•™ìŠµ í†µê³„
        self.stats = LearningStats()

        # íŒ¨í„´ ì¸ì‹
        self.successful_patterns: List[Dict] = []
        self.failed_patterns: List[Dict] = []

        # ì„±ê³¼ ì¶”ì 
        self.recent_rewards = deque(maxlen=100)

        self._load_data()

        logger.info(f"Self-Learning System initialized - Memory: {memory_size}, LR: {learning_rate}")

    def record_trade_experience(
        self,
        trade_id: str,
        stock_code: str,
        stock_name: str,
        entry_state: Dict[str, Any],
        action_params: Dict[str, Any],
        result: Dict[str, Any]
    ) -> float:
        """
        ê±°ë˜ ê²½í—˜ ê¸°ë¡ ë° í•™ìŠµ

        Args:
            trade_id: ê±°ë˜ ID
            stock_code: ì¢…ëª© ì½”ë“œ
            stock_name: ì¢…ëª©ëª…
            entry_state: ì§„ì… ì‹œ ìƒíƒœ
            action_params: í–‰ë™ íŒŒë¼ë¯¸í„°
            result: ê±°ë˜ ê²°ê³¼

        Returns:
            í•™ìŠµëœ Q-ê°’
        """
        # ë³´ìƒ ê³„ì‚°
        reward = self._calculate_reward(result)

        # ê²½í—˜ ìƒì„±
        experience = TradeExperience(
            trade_id=trade_id,
            timestamp=datetime.now(),
            stock_code=stock_code,
            stock_name=stock_name,
            state=entry_state,
            action=action_params,
            reward=reward,
            next_state=result.get('exit_state'),
            duration_hours=result.get('duration_hours', 0),
            max_drawdown=result.get('max_drawdown', 0),
            is_win=reward > 0
        )

        # ë©”ëª¨ë¦¬ì— ì €ì¥
        self.memory.append(experience)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_stats(experience)

        # Q-Learning ì—…ë°ì´íŠ¸
        q_value = self._update_q_table(experience)

        # íŒ¨í„´ í•™ìŠµ
        self._learn_pattern(experience)

        # ì£¼ê¸°ì  ì €ì¥ (100ê°œë§ˆë‹¤)
        if len(self.memory) % 100 == 0:
            self._save_data()

        # Epsilon ê°ì†Œ (íƒí—˜ â†’ í™œìš©)
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

        logger.info(
            f"ğŸ“š Learned from trade {trade_id}: "
            f"Reward={reward:.3f}, Q-value={q_value:.3f}, "
            f"Win={experience.is_win}"
        )

        return q_value

    def suggest_action(
        self,
        current_state: Dict[str, Any],
        available_actions: List[Dict[str, Any]]
    ) -> Tuple[Dict[str, Any], float]:
        """
        í˜„ì¬ ìƒíƒœì—ì„œ ìµœì  í–‰ë™ ì¶”ì²œ

        Args:
            current_state: í˜„ì¬ ìƒíƒœ
            available_actions: ê°€ëŠ¥í•œ í–‰ë™ë“¤

        Returns:
            (ì¶”ì²œ í–‰ë™, ì˜ˆìƒ Q-ê°’)
        """
        state_key = self._state_to_key(current_state)

        # Epsilon-Greedy ì „ëµ
        if np.random.random() < self.epsilon:
            # íƒí—˜: ëœë¤ ì„ íƒ
            action = np.random.choice(available_actions)
            q_value = self.q_table[state_key].get(self._action_to_key(action), 0.0)
            logger.debug(f"ğŸ” Exploration: Random action selected")
        else:
            # í™œìš©: ìµœê³  Q-ê°’ í–‰ë™ ì„ íƒ
            best_action = None
            best_q_value = -np.inf

            for action in available_actions:
                action_key = self._action_to_key(action)
                q_value = self.q_table[state_key].get(action_key, 0.0)

                if q_value > best_q_value:
                    best_q_value = q_value
                    best_action = action

            if best_action is None:
                best_action = available_actions[0] if available_actions else {}
                best_q_value = 0.0

            action = best_action
            q_value = best_q_value
            logger.debug(f"âœ¨ Exploitation: Best action selected (Q={q_value:.3f})")

        return action, q_value

    def get_learned_insights(self) -> Dict[str, Any]:
        """í•™ìŠµëœ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ"""
        # ê°€ì¥ ì„±ê³µì ì¸ íŒ¨í„´
        top_patterns = sorted(
            self.successful_patterns,
            key=lambda p: p.get('avg_reward', 0),
            reverse=True
        )[:5]

        # í”¼í•´ì•¼ í•  íŒ¨í„´
        worst_patterns = sorted(
            self.failed_patterns,
            key=lambda p: p.get('avg_reward', 0)
        )[:5]

        # ìµœê·¼ ì„±ê³¼
        recent_win_rate = (
            sum(1 for r in self.recent_rewards if r > 0) / len(self.recent_rewards)
            if self.recent_rewards else 0.5
        )

        # ê°€ì¥ ê°€ì¹˜ ìˆëŠ” ìƒíƒœ-í–‰ë™
        top_q_values = []
        for state_key, actions in self.q_table.items():
            for action_key, q_value in actions.items():
                if q_value > 0.1:  # ì˜ë¯¸ ìˆëŠ” ê°’ë§Œ
                    top_q_values.append({
                        'state': state_key,
                        'action': action_key,
                        'q_value': q_value,
                        'visit_count': self.visit_counts[state_key][action_key]
                    })

        top_q_values = sorted(top_q_values, key=lambda x: x['q_value'], reverse=True)[:10]

        insights = {
            'learning_stats': asdict(self.stats),
            'recent_win_rate': recent_win_rate,
            'avg_recent_reward': np.mean(self.recent_rewards) if self.recent_rewards else 0,
            'top_successful_patterns': top_patterns,
            'patterns_to_avoid': worst_patterns,
            'top_q_values': top_q_values,
            'exploration_rate': self.epsilon,
            'total_states_learned': len(self.q_table),
            'memory_usage': f"{len(self.memory)}/{self.memory.maxlen}"
        }

        return insights

    def get_adaptive_learning_rate(self) -> float:
        """ì ì‘í˜• í•™ìŠµë¥  ê³„ì‚°"""
        # ìµœê·¼ ì„±ê³¼ì— ë”°ë¼ í•™ìŠµë¥  ì¡°ì •
        if len(self.recent_rewards) < 10:
            return self.learning_rate

        recent_avg = np.mean(list(self.recent_rewards)[-20:])
        older_avg = np.mean(list(self.recent_rewards)[-40:-20]) if len(self.recent_rewards) >= 40 else recent_avg

        # ì„±ê³¼ ê°œì„  ì¤‘ì´ë©´ í•™ìŠµë¥  ìœ ì§€, ì•…í™”ë˜ë©´ ì¦ê°€
        if recent_avg > older_avg:
            # ê°œì„  ì¤‘: í˜„ì¬ í•™ìŠµ ìœ ì§€
            return self.learning_rate * 0.95
        else:
            # ì•…í™”: ë” ë¹ ë¥´ê²Œ í•™ìŠµ
            return min(self.learning_rate * 1.1, 0.3)

    def batch_learn_from_memory(self, batch_size: int = 32) -> float:
        """
        ë©”ëª¨ë¦¬ì—ì„œ ë°°ì¹˜ í•™ìŠµ (Experience Replay)

        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            í‰ê·  í•™ìŠµ ì˜¤ì°¨
        """
        if len(self.memory) < batch_size:
            return 0.0

        # ëœë¤ ìƒ˜í”Œë§
        indices = np.random.choice(len(self.memory), batch_size, replace=False)
        batch = [self.memory[i] for i in indices]

        total_error = 0.0

        for experience in batch:
            # Q-Learning ì—…ë°ì´íŠ¸
            error = self._update_q_table(experience)
            total_error += abs(error)

        avg_error = total_error / batch_size

        self.stats.learning_episodes += 1

        logger.info(f"ğŸ“– Batch learning completed: {batch_size} experiences, Avg error: {avg_error:.4f}")

        return avg_error

    def _calculate_reward(self, result: Dict[str, Any]) -> float:
        """
        ë³´ìƒ ê³„ì‚°

        ìˆ˜ìµë¥ , ë¦¬ìŠ¤í¬, ë³´ìœ  ê¸°ê°„ ë“±ì„ ê³ ë ¤í•œ ì¢…í•© ë³´ìƒ
        """
        profit_pct = result.get('profit_pct', 0.0)
        duration_hours = result.get('duration_hours', 24.0)
        max_drawdown = result.get('max_drawdown', 0.0)
        is_stopped = result.get('is_stopped', False)

        # ê¸°ë³¸ ë³´ìƒ: ìˆ˜ìµë¥ 
        reward = profit_pct

        # ì‹œê°„ ê°€ì¤‘ (ë¹ ë¥¸ ìˆ˜ìµ ì„ í˜¸)
        if profit_pct > 0:
            time_bonus = max(0, 1.0 - (duration_hours / 168))  # 1ì£¼ì¼ ê¸°ì¤€
            reward *= (1.0 + time_bonus * 0.5)

        # ë‚™í­ í˜ë„í‹°
        if max_drawdown < 0:
            reward += max_drawdown * 0.5  # ë‚™í­ì˜ ì ˆë°˜ë§Œí¼ ê°ì 

        # ì†ì ˆ í˜ë„í‹° ì™„í™” (ì†ì ˆì€ ì¢‹ì€ ê²ƒ)
        if is_stopped and profit_pct < 0:
            reward *= 0.7  # ì†ì ˆ ì‹œ ì†ì‹¤ 30% ê°ì†Œ

        # ì •ê·œí™” (-1 ~ 1 ë²”ìœ„)
        reward = np.tanh(reward * 5)  # tanhë¡œ ë²”ìœ„ ì œí•œ

        return reward

    def _update_q_table(self, experience: TradeExperience) -> float:
        """
        Q-í…Œì´ë¸” ì—…ë°ì´íŠ¸ (Q-Learning)

        Q(s,a) = Q(s,a) + Î± * [R + Î³ * max(Q(s',a')) - Q(s,a)]
        """
        state_key = self._state_to_key(experience.state)
        action_key = self._action_to_key(experience.action)

        # í˜„ì¬ Q-ê°’
        current_q = self.q_table[state_key][action_key]

        # ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Q-ê°’
        if experience.next_state:
            next_state_key = self._state_to_key(experience.next_state)
            max_next_q = max(self.q_table[next_state_key].values()) if self.q_table[next_state_key] else 0.0
        else:
            max_next_q = 0.0

        # ì ì‘í˜• í•™ìŠµë¥ 
        adaptive_lr = self.get_adaptive_learning_rate()

        # Q-Learning ì—…ë°ì´íŠ¸
        td_target = experience.reward + self.discount_factor * max_next_q
        td_error = td_target - current_q
        new_q = current_q + adaptive_lr * td_error

        self.q_table[state_key][action_key] = new_q

        # ë°©ë¬¸ íšŸìˆ˜ ì¦ê°€
        self.visit_counts[state_key][action_key] += 1

        return td_error

    def _learn_pattern(self, experience: TradeExperience):
        """íŒ¨í„´ í•™ìŠµ (ì„±ê³µ/ì‹¤íŒ¨ íŒ¨í„´ ì¶”ì¶œ)"""
        pattern = {
            'state_features': self._extract_features(experience.state),
            'action_params': experience.action,
            'reward': experience.reward,
            'count': 1
        }

        if experience.is_win:
            # ì„±ê³µ íŒ¨í„´
            self._add_to_patterns(pattern, self.successful_patterns)
        else:
            # ì‹¤íŒ¨ íŒ¨í„´
            self._add_to_patterns(pattern, self.failed_patterns)

    def _add_to_patterns(self, new_pattern: Dict, pattern_list: List[Dict]):
        """íŒ¨í„´ ëª©ë¡ì— ì¶”ê°€ (ìœ ì‚¬ íŒ¨í„´ ë³‘í•©)"""
        # ìœ ì‚¬ íŒ¨í„´ ì°¾ê¸°
        for existing in pattern_list:
            if self._is_similar_pattern(new_pattern, existing):
                # í‰ê·  ì—…ë°ì´íŠ¸
                total_count = existing['count'] + 1
                existing['avg_reward'] = (
                    existing.get('avg_reward', existing['reward']) * existing['count'] +
                    new_pattern['reward']
                ) / total_count
                existing['count'] = total_count
                return

        # ìƒˆ íŒ¨í„´ ì¶”ê°€
        new_pattern['avg_reward'] = new_pattern['reward']
        pattern_list.append(new_pattern)

        # ìµœëŒ€ 100ê°œ ìœ ì§€
        if len(pattern_list) > 100:
            pattern_list.pop(0)

    def _is_similar_pattern(self, pattern1: Dict, pattern2: Dict) -> bool:
        """íŒ¨í„´ ìœ ì‚¬ë„ íŒë‹¨"""
        # ê°„ë‹¨í•œ êµ¬í˜„: ìƒíƒœ íŠ¹ì§• ë¹„êµ
        features1 = pattern1.get('state_features', {})
        features2 = pattern2.get('state_features', {})

        # ëª‡ ê°œì˜ ì£¼ìš” íŠ¹ì§•ë§Œ ë¹„êµ
        key_features = ['volatility_level', 'trend', 'volume_level']
        matches = sum(1 for k in key_features if features1.get(k) == features2.get(k))

        return matches >= 2  # 3ê°œ ì¤‘ 2ê°œ ì´ìƒ ì¼ì¹˜

    def _extract_features(self, state: Dict[str, Any]) -> Dict[str, str]:
        """ìƒíƒœì—ì„œ ì£¼ìš” íŠ¹ì§• ì¶”ì¶œ"""
        volatility = state.get('volatility', 0.02)
        trend = state.get('trend', 0.0)
        volume_ratio = state.get('volume_ratio', 1.0)

        return {
            'volatility_level': 'high' if volatility > 0.03 else 'medium' if volatility > 0.015 else 'low',
            'trend': 'up' if trend > 0.02 else 'down' if trend < -0.02 else 'neutral',
            'volume_level': 'high' if volume_ratio > 1.5 else 'normal' if volume_ratio > 0.8 else 'low'
        }

    def _state_to_key(self, state: Dict[str, Any]) -> str:
        """ìƒíƒœë¥¼ í‚¤ë¡œ ë³€í™˜ (ì´ì‚°í™”)"""
        features = self._extract_features(state)
        return f"{features['volatility_level']}_{features['trend']}_{features['volume_level']}"

    def _action_to_key(self, action: Dict[str, Any]) -> str:
        """í–‰ë™ì„ í‚¤ë¡œ ë³€í™˜"""
        # ì£¼ìš” íŒŒë¼ë¯¸í„°ë§Œ ì‚¬ìš©
        position_size = action.get('position_size_pct', 0.1)
        stop_loss = action.get('stop_loss_pct', 0.05)

        pos_level = 'high' if position_size > 0.2 else 'medium' if position_size > 0.1 else 'low'
        sl_level = 'tight' if stop_loss < 0.04 else 'normal' if stop_loss < 0.08 else 'wide'

        return f"pos_{pos_level}_sl_{sl_level}"

    def _update_stats(self, experience: TradeExperience):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats.total_experiences += 1

        if experience.is_win:
            self.stats.total_wins += 1
        else:
            self.stats.total_losses += 1

        # í‰ê·  ë³´ìƒ ì—…ë°ì´íŠ¸
        self.stats.avg_reward = (
            self.stats.avg_reward * (self.stats.total_experiences - 1) +
            experience.reward
        ) / self.stats.total_experiences

        # ìµœê³ /ìµœì•… ë³´ìƒ
        self.stats.best_reward = max(self.stats.best_reward, experience.reward)
        self.stats.worst_reward = min(self.stats.worst_reward, experience.reward)

        self.stats.last_updated = datetime.now()

        # ìµœê·¼ ë³´ìƒ ì¶”ì 
        self.recent_rewards.append(experience.reward)

    def _save_data(self):
        """í•™ìŠµ ë°ì´í„° ì €ì¥"""
        try:
            data = {
                'stats': asdict(self.stats),
                'q_table': {
                    state: dict(actions)
                    for state, actions in self.q_table.items()
                },
                'visit_counts': {
                    state: dict(actions)
                    for state, actions in self.visit_counts.items()
                },
                'successful_patterns': self.successful_patterns[:50],  # ìƒìœ„ 50ê°œë§Œ
                'failed_patterns': self.failed_patterns[:50],
                'recent_rewards': list(self.recent_rewards),
                'epsilon': self.epsilon
            }

            with open(self.db_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False, default=str)

            logger.debug(f"Saved learning data: {len(self.q_table)} states")

        except Exception as e:
            logger.error(f"Failed to save learning data: {e}")

    def _load_data(self):
        """í•™ìŠµ ë°ì´í„° ë¡œë“œ"""
        try:
            if self.db_path.exists():
                with open(self.db_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # í†µê³„ ë³µì›
                stats_data = data.get('stats', {})
                if stats_data:
                    self.stats = LearningStats(**stats_data)

                # Q-í…Œì´ë¸” ë³µì›
                q_table_data = data.get('q_table', {})
                for state, actions in q_table_data.items():
                    self.q_table[state] = defaultdict(float, actions)

                # ë°©ë¬¸ íšŸìˆ˜ ë³µì›
                visit_data = data.get('visit_counts', {})
                for state, actions in visit_data.items():
                    self.visit_counts[state] = defaultdict(int, actions)

                # íŒ¨í„´ ë³µì›
                self.successful_patterns = data.get('successful_patterns', [])
                self.failed_patterns = data.get('failed_patterns', [])

                # ìµœê·¼ ë³´ìƒ ë³µì›
                recent = data.get('recent_rewards', [])
                self.recent_rewards.extend(recent)

                # Epsilon ë³µì›
                self.epsilon = data.get('epsilon', self.epsilon)

                logger.info(
                    f"Loaded learning data: {len(self.q_table)} states, "
                    f"{self.stats.total_experiences} experiences"
                )
        except Exception as e:
            logger.warning(f"Failed to load learning data: {e}")


# Singleton
_self_learning_system = None


def get_self_learning_system() -> SelfLearningSystem:
    """Get self-learning system singleton"""
    global _self_learning_system
    if _self_learning_system is None:
        _self_learning_system = SelfLearningSystem()
    return _self_learning_system


__all__ = ['SelfLearningSystem', 'get_self_learning_system', 'TradeExperience', 'LearningStats']
